{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3522656a-4bf6-4332-932a-6b81a0f2a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom Tokenizer Development\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "#mapping for contractions\n",
    "expand_map = {\n",
    "    r\"can't\": \"cannot\",\n",
    "    r\"won't\": \"will not\",\n",
    "    r\"i'm\": \"i am\",\n",
    "    r\"he's\": \"he is\",\n",
    "    r\"she's\": \"she is\",\n",
    "    r\"it's\": \"it is\",\n",
    "    r\"they're\": \"they are\",\n",
    "    r\"we're\": \"we are\",\n",
    "    r\"didn't\": \"did not\",\n",
    "    r\"don't\": \"do not\",\n",
    "    r\"isn't\": \"is not\",\n",
    "    r\"doesn't\": \"does not\",\n",
    "    r\"a\\*\\*\": \"ass\",\n",
    "    r\"f\\*+ck\": \"fuck\",\n",
    "    r\"b\\*tch\": \"bitch\",\n",
    "    r\"wasn't\": \"was not\",\n",
    "    r\"hasn't\": \"has not\",\n",
    "    r\"y'all\": \"you all\",\n",
    "    r\"you'll\": \"you will\", \n",
    "    r\"what's\": \"what is\",\n",
    "    r\"that's\": \"that is\",\n",
    "    r\"there's\": \"there is\",\n",
    "    r\"here's\": \"here is\",\n",
    "    r\"p\\*+ssy\": \"pussy\",\n",
    "    r\"sh\\*t\": \"shit\",\n",
    "}\n",
    "\n",
    "#mapping for common emoticons\n",
    "emoticon_map = {\n",
    "    r\":\\)\": \"<smile>\",\n",
    "    r\":-\\)\": \"<smile>\",\n",
    "    r\":\\(\": \"<sad>\",\n",
    "    r\":-\\(\": \"<sad>\",\n",
    "    r\";\\)\": \"<wink>\",\n",
    "    r\":D\": \"<laugh>\",\n",
    "}\n",
    "\n",
    "\n",
    "def expand(text): #removing contraction and handling informal language\n",
    "    for short_form, full_form in expand_map.items():\n",
    "        pattern = re.compile(re.escape(short_form), re.IGNORECASE)\n",
    "        text = pattern.sub(full_form, text)\n",
    "    return text   \n",
    "def emotions(text): #dealing with emoticons\n",
    "    for emoticon, token in emoticon_map.items():\n",
    "        text = re.sub(emoticon, token, text)    \n",
    "    return text    \n",
    "def normalize_repeats(word): #Repeated Character Normalization\n",
    "    match = re.search(r\"(.)\\1{2,}\", word)\n",
    "    if match:\n",
    "        char = match.group(1)\n",
    "        count = len(match.group(0))\n",
    "        base = re.sub(r\"(.)\\1{2,}\", char, word)\n",
    "        return f\"{base} <REPEAT:{count}>\"\n",
    "    return word\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = text.lower() # lowercase\n",
    "    text = expand(text) #Expand contractions\n",
    "    text = emotions(text) #Emoticons\n",
    "     \n",
    "    #punctuation\n",
    "    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
    "    text = re.sub(r'!{2,}', lambda m: f' <EXCLAMATION:{len(m.group())}> ', text)\n",
    "    text = re.sub(r'\\?{2,}', lambda m: f' <QUESTION:{len(m.group())}> ', text) \n",
    "\n",
    "    #creating tokens\n",
    "    tokens = text.split()\n",
    "    final_tokens = []\n",
    "    for tok in tokens:\n",
    "        if len(tok) > 2:\n",
    "            tok = normalize_repeats(tok)\n",
    "        final_tokens.extend(tok.split())  # to separate out <REPEAT:n>\n",
    "\n",
    "    return final_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb15159-cdff-45a1-b624-54eaeb1beaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ruleâ€‘Based POS Tagger\n",
    "\n",
    "def pos_tagger(final_tokens):\n",
    "    pos_tags = []\n",
    "    \n",
    "    for token in final_tokens:\n",
    "        if re.search(r\"(ing|ed|s)$\", token):\n",
    "            pos_tags.append((token, \"verb\"))\n",
    "        elif re.search(r\"(ly)$\", token):\n",
    "            pos_tags.append((token, \"adv\"))\n",
    "        elif re.search(r\"(ous|ful|able|ible|al|ic)$\", token):\n",
    "            pos_tags.append((token, \"adj\"))\n",
    "        elif token.lower() in [\"i\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\"]:\n",
    "            pos_tags.append((token, \"pron\"))\n",
    "        else:\n",
    "            pos_tags.append((token, \"noun\"))  #if none of above it will be noun\n",
    "            \n",
    "    return pos_tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb13307-5faf-4e25-bdd8-33bdbe04fa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom Stemmer\n",
    "\n",
    "def custom_stemmer(tagged_tokens):\n",
    "    Stemms = []\n",
    "    for word, tag in tagged_tokens:\n",
    "        stemm = word\n",
    "        if tag == \"verb\":\n",
    "            if word.endswith(\"ing\") and len(word) > 4:\n",
    "                stemm = word[:-3]\n",
    "            elif word.endswith(\"ed\") and len(word) > 3:\n",
    "                stemm = word[:-2]\n",
    "        elif tag == \"noun\":\n",
    "            if word.endswith(\"ness\"):\n",
    "                stemm = word[:-4]\n",
    "            elif word.endswith(\"s\") and len(word) > 3:\n",
    "                stemm = word[:-1]\n",
    "        elif tag == \"adj\":\n",
    "            if word.endswith(\"est\"):\n",
    "                stemm = word[:-3]\n",
    "            elif word.endswith(\"ous\"):\n",
    "                stemm = word[:-3]\n",
    "        stemms.append(stemm)\n",
    "    return stemms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d6fbed-0044-431f-a815-728c23e8b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extraction and Classification\n",
    "import pandas as pd\n",
    "\n",
    "#reading file\n",
    "fake = pd.read_csv(\"Fake.csv\")\n",
    "real = pd.read_csv(\"True.csv\")\n",
    "\n",
    "#inserting column and giving label as 0 or 1\n",
    "fake['label'] = 0\n",
    "real['label'] = 1\n",
    "\n",
    "#merging\n",
    "df = pd.concat([fake, real], axis=0).reset_index(drop=True)\n",
    "\n",
    "#merging the content in title and text\n",
    "df['content'] = df['title'] + \" \" + df['text']\n",
    "\n",
    "#generating tokens with simplification\n",
    "df['tokens'] = df['content'].apply(tokenizer)\n",
    "\n",
    "#building vocabulary using generated tokens\n",
    "def bvocab(tok_list):\n",
    "    vocab = set()\n",
    "    for tokens in tok_list:\n",
    "        vocab.update(tokens)\n",
    "    return sorted(list(vocab))\n",
    "    \n",
    "vocab = bvocab(df['tokens'])\n",
    "\n",
    "#vectorizing the generated vocabulary\n",
    "def vectorize(list, vocab):\n",
    "    vectors = []\n",
    "    for tokens in list:\n",
    "        vector = [0] * len(vocab)\n",
    "        for token in tokens:\n",
    "            if token in vocab:\n",
    "                idx = vocab.index(token)\n",
    "                vector[idx] += 1\n",
    "        vectors.append(vector)\n",
    "    return vectors\n",
    "\n",
    "#initiating inputs and labels\n",
    "X = vectorize(df['tokens'], vocab)\n",
    "y = df['label'].tolist() \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "#dividing the data in two random parts with 20% for testing and 80% for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#svm model training and prediction\n",
    "svm = LinearSVC()\n",
    "svm.fit(X_train, y_train)\n",
    "y_svm = svm.predict(X_test)\n",
    "\n",
    "#naive bayes model training and prediction\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_nb = nb.predict(X_test)\n",
    "\n",
    "#Comparing the performance of the two models on warious parameters\n",
    "def performance(y_true, y_pred, model_name):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    print(f\"{model_name} - Accuracy: {acc:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "performance(y_test, y_nb, \"Naive Bayes\")\n",
    "performance(y_test, y_svm, \"SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef1b476-58df-452c-9cc2-b32f57f68878",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualisations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce5746e-1578-4a52-aec9-e43a22eab6f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22b40d0-8258-435f-97e6-d0b89f1186c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
